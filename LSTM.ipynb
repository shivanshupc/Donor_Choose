{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment : 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n",
    "2. Load the data in your notebook.\n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum.\n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n",
    "8. Make sure that you are using GPU to train the given models.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can use gdown modules to import dataset for the assignment\n",
    "#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n",
    "#you can run the below cell to import the required preprocessed data.csv file and glove vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Model-1 </font>\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivanshu\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Shivanshu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Shivanshu\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import SpatialDropout1D, LSTM, BatchNormalization,concatenate,Flatten,Embedding,Dense,Dropout,MaxPooling2D,Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import he_normal,glorot_normal\n",
    "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import LeakyReLU,Concatenate\n",
    "from tensorflow.keras.initializers import he_normal,glorot_normal\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stratified train test split on the dataset\n",
    "X = df.drop(['project_is_approved'],axis=1)\n",
    "y = df['project_is_approved'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 8)   (87398,)\n",
      "(21850, 8)   (21850,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,' ',y_train.shape)\n",
    "print(X_test.shape,' ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
       "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
       "       'clean_categories', 'clean_subcategories', 'essay', 'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting y_train & y_test into categorical(One hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_binary_train = to_categorical(y_train, 2) \n",
    "y_binary_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51691\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train.essay)\n",
    "\n",
    "vocab_size_essay = len(tokenizer.word_index) + 1\n",
    "print(vocab_size_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    }
   ],
   "source": [
    "x_train_essay = tokenizer.texts_to_sequences(X_train.essay)\n",
    "x_test_essay = tokenizer.texts_to_sequences(X_test.essay)\n",
    "\n",
    "max_essay_length = max(len(elem) for elem in x_train_essay)\n",
    "print(max_essay_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# We pad our input sequences equal to the length of the longest Text sequence\n",
    "x_train_essay = pad_sequences(x_train_essay, maxlen=max_essay_length)\n",
    "x_test_essay = pad_sequences(x_test_essay, maxlen=max_essay_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "  model = pickle.load(f)\n",
    "  glove_words = set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size_essay, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "  if word in glove_words:\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Categorical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "lb.fit(X_train.school_state)\n",
    "\n",
    "x_train_ss = lb.transform(X_train.school_state)\n",
    "x_test_ss = lb.transform(X_test.school_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_school_state = len(lb.classes_)\n",
    "max_length_of_school_state_data = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "lb.fit(X_train.project_grade_category)\n",
    "\n",
    "x_train_grade = lb.transform(X_train.project_grade_category)\n",
    "x_test_grade = lb.transform(X_test.project_grade_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_project_grade_category = len(lb.classes_)\n",
    "max_length_of_project_grade_category_data = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "lb.fit(X_train.teacher_prefix)\n",
    "\n",
    "x_train_teacher = lb.transform(X_train.teacher_prefix)\n",
    "x_test_teacher = lb.transform(X_test.teacher_prefix)\n",
    "\n",
    "vocab_size_teacher_prefix = len(lb.classes_)\n",
    "max_length_of_teacher_prefix_data = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train.clean_categories)\n",
    "\n",
    "vocab_size_clean_categories = len(tokenizer.word_index)\n",
    "x_train_cc = tokenizer.texts_to_sequences(X_train.clean_categories)\n",
    "x_test_cc = tokenizer.texts_to_sequences(X_test.clean_categories)\n",
    "\n",
    "max_length_of_cat_data = max(len(elem) for elem in x_train_cc)\n",
    "print(max_length_of_cat_data)\n",
    "#padding\n",
    "x_train_cc = pad_sequences(x_train_cc, maxlen=max_length_of_cat_data)\n",
    "x_test_cc = pad_sequences(x_test_cc, maxlen=max_length_of_cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train.clean_subcategories)\n",
    "\n",
    "vocab_size_clean_subcategories = len(tokenizer.word_index)\n",
    "x_train_ccsub = tokenizer.texts_to_sequences(X_train.clean_subcategories)\n",
    "x_test_ccsub = tokenizer.texts_to_sequences(X_test.clean_subcategories)\n",
    "\n",
    "max_length_of_cat_ccsub_data = max(len(elem) for elem in x_train_ccsub)\n",
    "print(max_length_of_cat_ccsub_data)\n",
    "\n",
    "x_train_ccsub = pad_sequences(x_train_ccsub, maxlen=max_length_of_cat_ccsub_data)\n",
    "x_test_ccsub = pad_sequences(x_test_ccsub, maxlen=max_length_of_cat_ccsub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Numerical feature Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.teacher_number_of_previously_posted_projects.values.reshape(-1, 1))\n",
    "\n",
    "train_tnoppp = scaler.transform(X_train.teacher_number_of_previously_posted_projects.values.reshape(-1, 1))\n",
    "test_tnoppp = scaler.transform(X_test.teacher_number_of_previously_posted_projects.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.price.values.reshape(-1, 1))\n",
    "\n",
    "train_price = scaler.transform(X_train.price.values.reshape(-1, 1))\n",
    "test_price = scaler.transform(X_test.price.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_num_features = np.hstack((train_tnoppp, train_price))\n",
    "test_merged_num_features = np.hstack((test_tnoppp, test_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Custom roc_auc_score https://stackoverflow.com/a/51734992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc1(y_true, y_pred):\n",
    "    if len(np.unique(y_true[:,1])) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(auc1, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 339)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 339, 300)     15462900    ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " school_state_input (InputLayer  [(None, 1)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " project_grade_category_input (  [(None, 1)]         0           []                               \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " clean_categories_input (InputL  [(None, 3)]         0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " clean_subcategories_input (Inp  [(None, 3)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " teacher_prefix_input (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 339, 128)     219648      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 2)         102         ['school_state_input[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 2)         8           ['project_grade_category_input[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 3, 2)         18          ['clean_categories_input[0][0]'] \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 3, 2)         60          ['clean_subcategories_input[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 2)         10          ['teacher_prefix_input[0][0]']   \n",
      "                                                                                                  \n",
      " merged_num_features_input (Inp  [(None, 2)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 43392)        0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2)            0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2)            0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 6)            0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 6)            0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 2)            0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           60          ['merged_num_features_input[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 43430)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          5559168     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense_2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 2)            66          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,252,632\n",
      "Trainable params: 5,789,604\n",
      "Non-trainable params: 15,463,028\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_seq_total_text_data = Input(shape=(max_essay_length,), name=\"text_input\")\n",
    "embedding_layer = Embedding(vocab_size_essay, 300, weights=[embedding_matrix], input_length=max_essay_length, trainable=False)(Input_seq_total_text_data)\n",
    "lstm_out = LSTM(128, kernel_initializer='glorot_normal', kernel_regularizer=l2(0.0006),return_sequences=True)(embedding_layer)\n",
    "flatten_out_text_data = Flatten()(lstm_out)\n",
    "\n",
    "\n",
    "Input_school_state = Input(shape=(max_length_of_school_state_data,), name=\"school_state_input\")\n",
    "embedding_layer = Embedding(vocab_size_school_state, 2, input_length=1)(Input_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_grade_category = Input(shape=(max_length_of_project_grade_category_data,), name=\"project_grade_category_input\")\n",
    "embedding_layer = Embedding(vocab_size_project_grade_category, 2, input_length=1)(Input_grade_category)\n",
    "flatten_grade_category = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_teacher_prefix = Input(shape=(max_length_of_teacher_prefix_data,), name=\"teacher_prefix_input\")\n",
    "embedding_layer = Embedding(vocab_size_teacher_prefix, 2, input_length=1)(Input_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_clean_categories = Input(shape=(max_length_of_cat_data,), name=\"clean_categories_input\")\n",
    "embedding_layer = Embedding(vocab_size_clean_categories, 2, input_length=max_length_of_cat_data)(Input_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_clean_subcategories = Input(shape=(max_length_of_cat_ccsub_data,), name=\"clean_subcategories_input\")\n",
    "embedding_layer = Embedding(vocab_size_clean_subcategories, 2, input_length=max_length_of_cat_ccsub_data)(Input_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_merged_num_features = Input(shape=(2,), name=\"merged_num_features_input\")\n",
    "output_merged_num_features = Dense(20, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0003))(Input_merged_num_features)\n",
    "\n",
    "concat_layer = Concatenate()([flatten_out_text_data, flatten_school_state, flatten_grade_category, flatten_clean_categories, flatten_clean_subcategories, flatten_teacher_prefix, output_merged_num_features])\n",
    "\n",
    "# We stack a deep densely-connected network on top\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(concat_layer)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "# And finally we add the main softmax regression layer\n",
    "main_output = Dense(2, activation='softmax', kernel_initializer=\"glorot_uniform\", name='main_output')(x)\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\"model11_final.h5\",monitor=\"val_auroc\",mode=\"max\",save_best_only = True,verbose=1)\n",
    "earlystop1 = EarlyStopping(monitor = 'val_auroc', mode=\"max\",min_delta = 0, patience = 5,verbose = 1)\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=0, write_graph=True,write_grads=True)\n",
    "callbacks_1 = [tensorboard_callback,earlystop1,checkpoint1]\n",
    "\n",
    "model = Model(inputs=[Input_seq_total_text_data, Input_school_state, Input_grade_category, Input_clean_categories, Input_clean_subcategories, Input_teacher_prefix, Input_merged_num_features],outputs=main_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Compiling and fititng your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6291 - auroc: 0.6177\n",
      "Epoch 00001: val_auroc improved from -inf to 0.69812, saving model to model11_final.h5\n",
      "98/98 [==============================] - 38s 312ms/step - loss: 0.6291 - auroc: 0.6177 - val_loss: 0.5051 - val_auroc: 0.6981\n",
      "Epoch 2/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4816 - auroc: 0.7008\n",
      "Epoch 00002: val_auroc improved from 0.69812 to 0.70580, saving model to model11_final.h5\n",
      "98/98 [==============================] - 30s 306ms/step - loss: 0.4816 - auroc: 0.7008 - val_loss: 0.4688 - val_auroc: 0.7058\n",
      "Epoch 3/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4480 - auroc: 0.7182\n",
      "Epoch 00003: val_auroc improved from 0.70580 to 0.73377, saving model to model11_final.h5\n",
      "98/98 [==============================] - 30s 307ms/step - loss: 0.4480 - auroc: 0.7182 - val_loss: 0.4350 - val_auroc: 0.7338\n",
      "Epoch 4/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4303 - auroc: 0.7317\n",
      "Epoch 00004: val_auroc improved from 0.73377 to 0.73832, saving model to model11_final.h5\n",
      "98/98 [==============================] - 30s 307ms/step - loss: 0.4303 - auroc: 0.7317 - val_loss: 0.4222 - val_auroc: 0.7383\n",
      "Epoch 5/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4206 - auroc: 0.7420\n",
      "Epoch 00005: val_auroc did not improve from 0.73832\n",
      "98/98 [==============================] - 30s 304ms/step - loss: 0.4206 - auroc: 0.7420 - val_loss: 0.4267 - val_auroc: 0.7377\n",
      "Epoch 6/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4134 - auroc: 0.7491\n",
      "Epoch 00006: val_auroc improved from 0.73832 to 0.74827, saving model to model11_final.h5\n",
      "98/98 [==============================] - 30s 304ms/step - loss: 0.4134 - auroc: 0.7491 - val_loss: 0.4160 - val_auroc: 0.7483\n",
      "Epoch 7/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4057 - auroc: 0.7631\n",
      "Epoch 00007: val_auroc did not improve from 0.74827\n",
      "98/98 [==============================] - 30s 301ms/step - loss: 0.4057 - auroc: 0.7631 - val_loss: 0.4129 - val_auroc: 0.7474\n",
      "Epoch 8/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4005 - auroc: 0.7755\n",
      "Epoch 00008: val_auroc did not improve from 0.74827\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 0.4005 - auroc: 0.7755 - val_loss: 0.4338 - val_auroc: 0.7267\n",
      "Epoch 9/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3967 - auroc: 0.7924\n",
      "Epoch 00009: val_auroc did not improve from 0.74827\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 0.3967 - auroc: 0.7924 - val_loss: 0.4221 - val_auroc: 0.7439\n",
      "Epoch 10/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3934 - auroc: 0.8079\n",
      "Epoch 00010: val_auroc did not improve from 0.74827\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 0.3934 - auroc: 0.8079 - val_loss: 0.4399 - val_auroc: 0.7330\n",
      "Epoch 11/40\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3886 - auroc: 0.8305\n",
      "Epoch 00011: val_auroc did not improve from 0.74827\n",
      "98/98 [==============================] - 29s 293ms/step - loss: 0.3886 - auroc: 0.8305 - val_loss: 0.4618 - val_auroc: 0.7216\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0009), loss='categorical_crossentropy', metrics=[auroc])\n",
    "history = model.fit([x_train_essay, x_train_ss, x_train_grade, x_train_cc, x_train_ccsub, x_train_teacher, train_merged_num_features], y_binary_train,batch_size=900,epochs=40,callbacks=callbacks_1,validation_data=([x_test_essay, x_test_ss, x_test_grade, x_test_cc, x_test_ccsub, x_test_teacher, test_merged_num_features], y_binary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 82ms/step - loss: 0.4618 - auroc: 0.7221\n",
      "Test loss: 0.46183380484580994\n",
      "Test AUC: 0.7221055030822754\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([x_test_essay, x_test_ss, x_test_grade, x_test_cc, x_test_ccsub, x_test_teacher, test_merged_num_features], y_binary_test, batch_size=700)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test AUC:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 </br>\n",
    "- Test AUC = 0.72\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Fit TF-IDF vectorizer on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n",
    "\n",
    "3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \n",
    "frequent words and very very rare words don't give much information.\n",
    "Hint - A preferable IDF range is 2-11 for model 2. <br>\n",
    "4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\n",
    "sentence of train and test data and include only those features(words) which are present in the defined IDF range.\n",
    "5. Perform tokenization on the modified text data same as you have done for previous model.\n",
    "6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n",
    "7. Define the model, compile and fit the model.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X_train.essay)\n",
    "data = {'word': vectorizer.get_feature_names() , 'idf_value': vectorizer.idf_}\n",
    "tfidf_df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min tf-idf :  1.0077877719582724\n",
      "Max tf-idf :  11.685091939370627\n"
     ]
    }
   ],
   "source": [
    "print(\"Min tf-idf : \",min(tfidf_df['idf_value']))\n",
    "print(\"Max tf-idf : \",max(tfidf_df['idf_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31444, 2)\n"
     ]
    }
   ],
   "source": [
    "filter =   (tfidf_df['idf_value']>=1.007) & (tfidf_df['idf_value'] <=11.68)\n",
    "tfidf_best = tfidf_df[filter]\n",
    "print(tfidf_best.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_tfidf = tfidf_best['word'].tolist()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(Best_tfidf)\n",
    "\n",
    "X_train_essay_tfidf  = tokenizer.texts_to_sequences(X_train.essay)\n",
    "X_test_essay_tfidf   = tokenizer.texts_to_sequences(X_test.essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "vocab_size_essay = len(tokenizer.word_index) + 1\n",
    "max_essay_length_tfidf = max(len(elem) for elem in X_train_essay_tfidf)\n",
    "print(max_essay_length_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_essay_tfidf = pad_sequences(X_train_essay_tfidf, maxlen=max_essay_length_tfidf)\n",
    "X_test_essay_tfidf = pad_sequences(X_test_essay_tfidf, maxlen=max_essay_length_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "  model = pickle.load(f)\n",
    "  glove_words = set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size_essay, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "  if word in glove_words:\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 329)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 329, 300)     9433500     ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, 329, 300)    0           ['embedding[0][0]']              \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " school_state_input (InputLayer  [(None, 1)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " project_grade_category_input (  [(None, 1)]         0           []                               \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " clean_categories_input (InputL  [(None, 3)]         0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " clean_subcategories_input (Inp  [(None, 3)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " teacher_prefix_input (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 329, 128)     219648      ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 2)         102         ['school_state_input[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 2)         8           ['project_grade_category_input[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 3, 2)         18          ['clean_categories_input[0][0]'] \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 3, 2)         60          ['clean_subcategories_input[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 2)         10          ['teacher_prefix_input[0][0]']   \n",
      "                                                                                                  \n",
      " merged_num_features_input (Inp  [(None, 2)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 42112)        0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2)            0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2)            0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 6)            0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 6)            0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 2)            0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           48          ['merged_num_features_input[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 42146)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          5394816     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense_2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 2)            66          ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,058,868\n",
      "Trainable params: 5,625,240\n",
      "Non-trainable params: 9,433,628\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_seq_total_text_data = Input(shape=(max_essay_length_tfidf,), name=\"text_input\")\n",
    "embedding_layer = Embedding(vocab_size_essay, 300, weights=[embedding_matrix], input_length=max_essay_length_tfidf, trainable=False)(Input_seq_total_text_data)\n",
    "embedding_layer = SpatialDropout1D(0.3)(embedding_layer)\n",
    "lstm_out = LSTM(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.0001),return_sequences=True)(embedding_layer)\n",
    "flatten_out_text_data = Flatten()(lstm_out)\n",
    "\n",
    "Input_school_state = Input(shape=(max_length_of_school_state_data,), name=\"school_state_input\")\n",
    "embedding_layer = Embedding(vocab_size_school_state, 2, input_length=1)(Input_school_state)\n",
    "flatten_school_state = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_grade_category = Input(shape=(max_length_of_project_grade_category_data,), name=\"project_grade_category_input\")\n",
    "embedding_layer = Embedding(vocab_size_project_grade_category, 2, input_length=1)(Input_grade_category)\n",
    "flatten_grade_category = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_teacher_prefix = Input(shape=(max_length_of_teacher_prefix_data,), name=\"teacher_prefix_input\")\n",
    "embedding_layer = Embedding(vocab_size_teacher_prefix, 2, input_length=1)(Input_teacher_prefix)\n",
    "flatten_teacher_prefix = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_clean_categories = Input(shape=(max_length_of_cat_data,), name=\"clean_categories_input\")\n",
    "embedding_layer = Embedding(vocab_size_clean_categories, 2, input_length=max_length_of_cat_data)(Input_clean_categories)\n",
    "flatten_clean_categories = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_clean_subcategories = Input(shape=(max_length_of_cat_ccsub_data,), name=\"clean_subcategories_input\")\n",
    "embedding_layer = Embedding(vocab_size_clean_subcategories, 2, input_length=max_length_of_cat_ccsub_data)(Input_clean_subcategories)\n",
    "flatten_clean_subcategories = Flatten()(embedding_layer)\n",
    "\n",
    "\n",
    "Input_merged_num_features = Input(shape=(2,), name=\"merged_num_features_input\")\n",
    "output_merged_num_features = Dense(16, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(Input_merged_num_features)\n",
    "\n",
    "concat_layer = Concatenate()([flatten_out_text_data, flatten_school_state, flatten_grade_category, flatten_clean_categories, flatten_clean_subcategories, flatten_teacher_prefix, output_merged_num_features])\n",
    "\n",
    "# stack deep densely-connected network on top\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(concat_layer)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "# the main softmax regression layer\n",
    "main_output = Dense(2, activation='softmax', kernel_initializer=\"glorot_uniform\", name='main_output')(x)\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\"model21_final.h5\",monitor=\"val_auroc\",mode=\"max\",save_best_only = True,verbose=1)\n",
    "earlystop1 = EarlyStopping(monitor = 'val_auroc', mode=\"max\",min_delta = 0, patience = 5,verbose = 1)\n",
    "tensorboard1 = TensorBoard(log_dir='logs/train')\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=0, write_graph=True,write_grads=True)\n",
    "callbacks_1 = [tensorboard_callback,earlystop1,checkpoint1]\n",
    "\n",
    "\n",
    "model2_1 = Model(inputs=[Input_seq_total_text_data, Input_school_state, Input_grade_category, Input_clean_categories, Input_clean_subcategories, Input_teacher_prefix, Input_merged_num_features],outputs=main_output)\n",
    "model2_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7163 - auroc: 0.5175\n",
      "Epoch 00001: val_auroc improved from -inf to 0.57519, saving model to model21_final.h5\n",
      "110/110 [==============================] - 35s 280ms/step - loss: 0.7163 - auroc: 0.5175 - val_loss: 0.5682 - val_auroc: 0.5752\n",
      "Epoch 2/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5801 - auroc: 0.5208\n",
      "Epoch 00002: val_auroc did not improve from 0.57519\n",
      "110/110 [==============================] - 30s 273ms/step - loss: 0.5801 - auroc: 0.5208 - val_loss: 0.5403 - val_auroc: 0.5274\n",
      "Epoch 3/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5387 - auroc: 0.5277\n",
      "Epoch 00003: val_auroc did not improve from 0.57519\n",
      "110/110 [==============================] - 30s 274ms/step - loss: 0.5387 - auroc: 0.5277 - val_loss: 0.5197 - val_auroc: 0.4570\n",
      "Epoch 4/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.5019 - auroc: 0.5857\n",
      "Epoch 00004: val_auroc improved from 0.57519 to 0.68741, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 278ms/step - loss: 0.5019 - auroc: 0.5857 - val_loss: 0.5037 - val_auroc: 0.6874\n",
      "Epoch 5/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4741 - auroc: 0.6449\n",
      "Epoch 00005: val_auroc improved from 0.68741 to 0.71357, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4741 - auroc: 0.6449 - val_loss: 0.4432 - val_auroc: 0.7136\n",
      "Epoch 6/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4582 - auroc: 0.6680\n",
      "Epoch 00006: val_auroc improved from 0.71357 to 0.72772, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4582 - auroc: 0.6680 - val_loss: 0.4534 - val_auroc: 0.7277\n",
      "Epoch 7/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4442 - auroc: 0.6867\n",
      "Epoch 00007: val_auroc improved from 0.72772 to 0.73060, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4442 - auroc: 0.6867 - val_loss: 0.4243 - val_auroc: 0.7306\n",
      "Epoch 8/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4351 - auroc: 0.6995\n",
      "Epoch 00008: val_auroc improved from 0.73060 to 0.73922, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4351 - auroc: 0.6995 - val_loss: 0.4150 - val_auroc: 0.7392\n",
      "Epoch 9/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4269 - auroc: 0.7070\n",
      "Epoch 00009: val_auroc improved from 0.73922 to 0.74384, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4269 - auroc: 0.7070 - val_loss: 0.4118 - val_auroc: 0.7438\n",
      "Epoch 10/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4215 - auroc: 0.7105\n",
      "Epoch 00010: val_auroc improved from 0.74384 to 0.74815, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4215 - auroc: 0.7105 - val_loss: 0.4096 - val_auroc: 0.7482\n",
      "Epoch 11/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4167 - auroc: 0.7173\n",
      "Epoch 00011: val_auroc did not improve from 0.74815\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.4167 - auroc: 0.7173 - val_loss: 0.4024 - val_auroc: 0.7470\n",
      "Epoch 12/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4108 - auroc: 0.7256\n",
      "Epoch 00012: val_auroc improved from 0.74815 to 0.75154, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4108 - auroc: 0.7256 - val_loss: 0.4016 - val_auroc: 0.7515\n",
      "Epoch 13/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4071 - auroc: 0.7275\n",
      "Epoch 00013: val_auroc improved from 0.75154 to 0.75471, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4071 - auroc: 0.7275 - val_loss: 0.4071 - val_auroc: 0.7547\n",
      "Epoch 14/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4038 - auroc: 0.7330\n",
      "Epoch 00014: val_auroc improved from 0.75471 to 0.75479, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4038 - auroc: 0.7330 - val_loss: 0.3965 - val_auroc: 0.7548\n",
      "Epoch 15/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4021 - auroc: 0.7335\n",
      "Epoch 00015: val_auroc improved from 0.75479 to 0.75524, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.4021 - auroc: 0.7335 - val_loss: 0.3962 - val_auroc: 0.7552\n",
      "Epoch 16/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3980 - auroc: 0.7406\n",
      "Epoch 00016: val_auroc improved from 0.75524 to 0.75965, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3980 - auroc: 0.7406 - val_loss: 0.3938 - val_auroc: 0.7596\n",
      "Epoch 17/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3959 - auroc: 0.7440\n",
      "Epoch 00017: val_auroc did not improve from 0.75965\n",
      "110/110 [==============================] - 31s 278ms/step - loss: 0.3959 - auroc: 0.7440 - val_loss: 0.3916 - val_auroc: 0.7587\n",
      "Epoch 18/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3939 - auroc: 0.7470\n",
      "Epoch 00018: val_auroc did not improve from 0.75965\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3939 - auroc: 0.7470 - val_loss: 0.3887 - val_auroc: 0.7575\n",
      "Epoch 19/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3922 - auroc: 0.7482\n",
      "Epoch 00019: val_auroc improved from 0.75965 to 0.75986, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3922 - auroc: 0.7482 - val_loss: 0.3861 - val_auroc: 0.7599\n",
      "Epoch 20/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3914 - auroc: 0.7485\n",
      "Epoch 00020: val_auroc improved from 0.75986 to 0.76250, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3914 - auroc: 0.7485 - val_loss: 0.3942 - val_auroc: 0.7625\n",
      "Epoch 21/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3906 - auroc: 0.7497\n",
      "Epoch 00021: val_auroc improved from 0.76250 to 0.76385, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3906 - auroc: 0.7497 - val_loss: 0.3918 - val_auroc: 0.7638\n",
      "Epoch 22/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3883 - auroc: 0.7547\n",
      "Epoch 00022: val_auroc did not improve from 0.76385\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3883 - auroc: 0.7547 - val_loss: 0.3865 - val_auroc: 0.7629\n",
      "Epoch 23/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3868 - auroc: 0.7579\n",
      "Epoch 00023: val_auroc improved from 0.76385 to 0.76464, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3868 - auroc: 0.7579 - val_loss: 0.3877 - val_auroc: 0.7646\n",
      "Epoch 24/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3861 - auroc: 0.7576\n",
      "Epoch 00024: val_auroc improved from 0.76464 to 0.76474, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3861 - auroc: 0.7576 - val_loss: 0.3878 - val_auroc: 0.7647\n",
      "Epoch 25/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3841 - auroc: 0.7623\n",
      "Epoch 00025: val_auroc improved from 0.76474 to 0.76600, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3841 - auroc: 0.7623 - val_loss: 0.3861 - val_auroc: 0.7660\n",
      "Epoch 26/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3831 - auroc: 0.7646\n",
      "Epoch 00026: val_auroc did not improve from 0.76600\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3831 - auroc: 0.7646 - val_loss: 0.3874 - val_auroc: 0.7628\n",
      "Epoch 27/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3820 - auroc: 0.7693\n",
      "Epoch 00027: val_auroc did not improve from 0.76600\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3820 - auroc: 0.7693 - val_loss: 0.3928 - val_auroc: 0.7633\n",
      "Epoch 28/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3839 - auroc: 0.7657\n",
      "Epoch 00028: val_auroc did not improve from 0.76600\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3839 - auroc: 0.7657 - val_loss: 0.3934 - val_auroc: 0.7602\n",
      "Epoch 29/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3824 - auroc: 0.7705\n",
      "Epoch 00029: val_auroc improved from 0.76600 to 0.76730, saving model to model21_final.h5\n",
      "110/110 [==============================] - 31s 279ms/step - loss: 0.3824 - auroc: 0.7705 - val_loss: 0.3972 - val_auroc: 0.7673\n",
      "Epoch 30/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3814 - auroc: 0.7733\n",
      "Epoch 00030: val_auroc did not improve from 0.76730\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3814 - auroc: 0.7733 - val_loss: 0.3939 - val_auroc: 0.7653\n",
      "Epoch 31/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3813 - auroc: 0.7750\n",
      "Epoch 00031: val_auroc did not improve from 0.76730\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3813 - auroc: 0.7750 - val_loss: 0.3922 - val_auroc: 0.7640\n",
      "Epoch 32/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3793 - auroc: 0.7827\n",
      "Epoch 00032: val_auroc did not improve from 0.76730\n",
      "110/110 [==============================] - 31s 277ms/step - loss: 0.3793 - auroc: 0.7827 - val_loss: 0.3962 - val_auroc: 0.7618\n",
      "Epoch 33/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3794 - auroc: 0.7833\n",
      "Epoch 00033: val_auroc did not improve from 0.76730\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3794 - auroc: 0.7833 - val_loss: 0.4012 - val_auroc: 0.7622\n",
      "Epoch 34/50\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.3786 - auroc: 0.7877\n",
      "Epoch 00034: val_auroc did not improve from 0.76730\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.3786 - auroc: 0.7877 - val_loss: 0.4028 - val_auroc: 0.7623\n",
      "Epoch 00034: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2_1.compile(optimizer=Adam(learning_rate=0.0006), loss='categorical_crossentropy', metrics=[auroc])\n",
    "history2_1 = model2_1.fit([X_train_essay_tfidf, x_train_ss, x_train_grade, x_train_cc, x_train_ccsub, x_train_teacher, train_merged_num_features], y_binary_train,batch_size=800,epochs=50,callbacks=callbacks_1,validation_data=([X_test_essay_tfidf, x_test_ss, x_test_grade, x_test_cc, x_test_ccsub, x_test_teacher, test_merged_num_features], y_binary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 2s 78ms/step - loss: 0.4028 - auroc: 0.7623\n",
      "Test loss: 0.4028013348579407\n",
      "Test AUC: 0.7622679471969604\n"
     ]
    }
   ],
   "source": [
    "score = model2_1.evaluate([X_test_essay_tfidf, x_test_ss, x_test_grade, x_test_cc, x_test_ccsub, x_test_teacher, test_merged_num_features], y_binary_test, batch_size=800)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test AUC:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 </br>\n",
    "- Test AUC : 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Model-3 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['school_state'].values)\n",
    "x_train_state_one_hot = vectorizer.transform(X_train['school_state'].values)\n",
    "x_test_state_one_hot = vectorizer.transform(X_test['school_state'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['clean_categories'].values)\n",
    "x_train_categories_one_hot = vectorizer.transform(X_train['clean_categories'].values)\n",
    "x_test_categories_one_hot = vectorizer.transform(X_test['clean_categories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['clean_subcategories'].values)\n",
    "x_train_subcategories_one_hot = vectorizer.transform(X_train['clean_subcategories'].values)\n",
    "x_test_subcategories_one_hot = vectorizer.transform(X_test['clean_subcategories'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['teacher_prefix'].values)\n",
    "x_train_teacher_prefix_one_hot = vectorizer.transform(X_train['teacher_prefix'].values)\n",
    "x_test_teacher_prefix_one_hot = vectorizer.transform(X_test['teacher_prefix'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['project_grade_category'].values)\n",
    "x_train_project_grade_one_hot = vectorizer.transform(X_train['project_grade_category'].values)\n",
    "x_test_project_grade_one_hot = vectorizer.transform(X_test['project_grade_category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "train_features_withouttext = hstack((x_train_project_grade_one_hot,x_train_teacher_prefix_one_hot,x_train_categories_one_hot,x_train_subcategories_one_hot,x_train_state_one_hot,train_price,train_tnoppp)).todense()\n",
    "test_features_withouttext = hstack((x_test_project_grade_one_hot,x_test_teacher_prefix_one_hot,x_test_categories_one_hot,x_test_subcategories_one_hot,x_test_state_one_hot,test_price,test_tnoppp)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data  (87398, 101, 1)\n",
      "Test data s (21850, 101, 1)\n"
     ]
    }
   ],
   "source": [
    "rest_features_train = np.expand_dims(train_features_withouttext,2)\n",
    "rest_features_test = np.expand_dims(test_features_withouttext,2)\n",
    "print(\"Train data \",rest_features_train.shape)\n",
    "print(\"Test data s\",rest_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - batch Size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 339)]        0           []                               \n",
      "                                                                                                  \n",
      " without_text (InputLayer)      [(None, 101, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 339, 300)     15507300    ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 99, 128)      512         ['without_text[0][0]']           \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 339, 20)      25680       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 97, 128)      49280       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 6780)         0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 12416)        0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 19196)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           383940      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 20)           420         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 20)          80          ['dense_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 20)           420         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            42          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,967,674\n",
      "Trainable params: 460,334\n",
      "Non-trainable params: 15,507,340\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_seq_total_text_data = Input(shape=(max_essay_length,), name=\"text_input\")\n",
    "embedding_layer = Embedding(vocab_size_essay, 300, weights=[embedding_matrix], input_length=max_essay_length, trainable=False)(Input_seq_total_text_data)\n",
    "lstm_out = LSTM(20, kernel_initializer='glorot_normal', kernel_regularizer=l1_l2(0.001),return_sequences=True)(embedding_layer)\n",
    "flatten_out_text_data = Flatten()(lstm_out)\n",
    "\n",
    "input_without_text  = Input(shape=(101,1), name='without_text')\n",
    "convo = Conv1D(128 , 3 , activation='relu' ,  kernel_initializer=he_normal(seed=None) , padding='valid')(input_without_text)\n",
    "convo = Conv1D(128 , 3 , activation='relu' ,  kernel_initializer=he_normal(seed=None) , padding='valid')(convo)\n",
    "flatten_without_text = Flatten()(convo)\n",
    "\n",
    "x_concat = concatenate([flatten_out_text_data  , flatten_without_text])\n",
    "\n",
    "x = Dense(20, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(x_concat)\n",
    "x=Dropout(0.1)(x)\n",
    "x = Dense(20,activation=\"sigmoid\",kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x=Dropout(0.1)(x)\n",
    "x = Dense(20,activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.001))(x)\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output')(x)\n",
    "model_3_1 = Model(inputs=[Input_seq_total_text_data, input_without_text],outputs=[output])\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\"model3_1.h5\",monitor=\"val_auroc\",mode=\"max\",save_best_only = True,verbose=1)\n",
    "earlystop1 = EarlyStopping(monitor = 'val_auroc', mode=\"max\",min_delta = 0, patience = 5,verbose = 1)\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=0, write_graph=True,write_grads=True)\n",
    "callbacks_1 = [tensorboard_callback,earlystop1,checkpoint1]\n",
    "\n",
    "model_3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.6020 - auroc: 0.5348\n",
      "Epoch 00001: val_auroc improved from -inf to 0.56128, saving model to model3_1.h5\n",
      "1366/1366 [==============================] - 35s 24ms/step - loss: 0.6020 - auroc: 0.5348 - val_loss: 0.4602 - val_auroc: 0.5613\n",
      "Epoch 2/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4508 - auroc: 0.5659\n",
      "Epoch 00002: val_auroc did not improve from 0.56128\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4508 - auroc: 0.5659 - val_loss: 0.4468 - val_auroc: 0.5398\n",
      "Epoch 3/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4369 - auroc: 0.5917\n",
      "Epoch 00003: val_auroc improved from 0.56128 to 0.66517, saving model to model3_1.h5\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4369 - auroc: 0.5917 - val_loss: 0.4486 - val_auroc: 0.6652\n",
      "Epoch 4/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4145 - auroc: 0.7046\n",
      "Epoch 00004: val_auroc improved from 0.66517 to 0.72347, saving model to model3_1.h5\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4145 - auroc: 0.7046 - val_loss: 0.4337 - val_auroc: 0.7235\n",
      "Epoch 5/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4037 - auroc: 0.7169\n",
      "Epoch 00005: val_auroc improved from 0.72347 to 0.72885, saving model to model3_1.h5\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4037 - auroc: 0.7169 - val_loss: 0.4082 - val_auroc: 0.7289\n",
      "Epoch 6/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4000 - auroc: 0.7194\n",
      "Epoch 00006: val_auroc did not improve from 0.72885\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4000 - auroc: 0.7194 - val_loss: 0.4348 - val_auroc: 0.7235\n",
      "Epoch 7/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.4001 - auroc: 0.7207\n",
      "Epoch 00007: val_auroc did not improve from 0.72885\n",
      "1366/1366 [==============================] - 30s 22ms/step - loss: 0.4001 - auroc: 0.7207 - val_loss: 0.4017 - val_auroc: 0.7263\n",
      "Epoch 8/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.3988 - auroc: 0.7236\n",
      "Epoch 00008: val_auroc did not improve from 0.72885\n",
      "1366/1366 [==============================] - 29s 21ms/step - loss: 0.3988 - auroc: 0.7236 - val_loss: 0.3984 - val_auroc: 0.7280\n",
      "Epoch 9/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.3977 - auroc: 0.7218\n",
      "Epoch 00009: val_auroc did not improve from 0.72885\n",
      "1366/1366 [==============================] - 29s 21ms/step - loss: 0.3977 - auroc: 0.7218 - val_loss: 0.4066 - val_auroc: 0.7265\n",
      "Epoch 10/35\n",
      "1366/1366 [==============================] - ETA: 0s - loss: 0.3974 - auroc: 0.7258\n",
      "Epoch 00010: val_auroc did not improve from 0.72885\n",
      "1366/1366 [==============================] - 29s 21ms/step - loss: 0.3974 - auroc: 0.7258 - val_loss: 0.4186 - val_auroc: 0.7213\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_3_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[auroc])\n",
    "history_3_1 = model_3_1.fit([x_train_essay,rest_features_train], y_binary_train, batch_size=64, epochs=35, verbose=1,callbacks=callbacks_1, validation_data=([x_test_essay,rest_features_test], y_binary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 4s 12ms/step - loss: 0.4186 - auroc: 0.7213\n",
      "Test loss: 0.4185788631439209\n",
      "Test AUC: 0.7212847471237183\n"
     ]
    }
   ],
   "source": [
    "score = model_3_1.evaluate([x_test_essay,rest_features_test], y_binary_test, batch_size=64)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test AUC:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - batch size = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None, 339)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 339, 300)     15507300    ['text_input[0][0]']             \n",
      "                                                                                                  \n",
      " without_text (InputLayer)      [(None, 101, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, 339, 300)    0           ['embedding[0][0]']              \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 94, 100)      900         ['without_text[0][0]']           \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 339, 128)     219648      ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 87, 80)       64080       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 43392)        0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 6960)         0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50352)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          6445184     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64)          256         ['dense_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            66          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,247,770\n",
      "Trainable params: 6,740,342\n",
      "Non-trainable params: 15,507,428\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_seq_total_text_data = Input(shape=(max_essay_length,), name=\"text_input\")\n",
    "embedding_layer = Embedding(vocab_size_essay, 300, weights=[embedding_matrix], input_length=max_essay_length, trainable=False)(Input_seq_total_text_data)\n",
    "embedding_layer = SpatialDropout1D(0.15)(embedding_layer)\n",
    "lstm_out = LSTM(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.0007),return_sequences=True)(embedding_layer)\n",
    "flatten_out_text_data = Flatten()(lstm_out)\n",
    "\n",
    "input_without_text  = Input(shape=(101,1), name='without_text')\n",
    "convo = Conv1D(100 , 8 , activation='relu' ,  kernel_initializer=he_normal(seed=None) , padding='valid')(input_without_text)\n",
    "convo = Conv1D(80 , 8 , activation='relu' ,  kernel_initializer=he_normal(seed=None) , padding='valid')(convo)\n",
    "flatten_without_text = Flatten()(convo)\n",
    "\n",
    "x_concat = concatenate([flatten_out_text_data  , flatten_without_text])\n",
    "\n",
    "x = Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.0005))(x_concat)\n",
    "x=Dropout(0.15)(x)\n",
    "x = Dense(64,activation=\"sigmoid\",kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.0004))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x=Dropout(0.2)(x)\n",
    "x = Dense(32,activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output')(x)\n",
    "model_3_2 = Model(inputs=[Input_seq_total_text_data, input_without_text],outputs=[output])\n",
    "\n",
    "checkpoint1 = ModelCheckpoint(\"model32.h5\",monitor=\"val_auroc\",mode=\"max\",save_best_only = True,verbose=1)\n",
    "earlystop1 = EarlyStopping(monitor = 'val_auroc', mode=\"max\",min_delta = 0, patience = 5,verbose = 1)\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=0, write_graph=True,write_grads=True)\n",
    "callbacks_1 = [tensorboard_callback,earlystop1,checkpoint1]\n",
    "\n",
    "model_3_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 1.0722 - auroc: 0.5246\n",
      "Epoch 00001: val_auroc improved from -inf to 0.52122, saving model to model32.h5\n",
      "98/98 [==============================] - 43s 349ms/step - loss: 1.0722 - auroc: 0.5246 - val_loss: 0.7761 - val_auroc: 0.5212\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.6394 - auroc: 0.5974\n",
      "Epoch 00002: val_auroc improved from 0.52122 to 0.68122, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 329ms/step - loss: 0.6394 - auroc: 0.5974 - val_loss: 0.5516 - val_auroc: 0.6812\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.5125 - auroc: 0.6688\n",
      "Epoch 00003: val_auroc improved from 0.68122 to 0.71076, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 330ms/step - loss: 0.5125 - auroc: 0.6688 - val_loss: 0.4726 - val_auroc: 0.7108\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4684 - auroc: 0.6921\n",
      "Epoch 00004: val_auroc improved from 0.71076 to 0.72124, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 331ms/step - loss: 0.4684 - auroc: 0.6921 - val_loss: 0.4584 - val_auroc: 0.7212\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4476 - auroc: 0.7058\n",
      "Epoch 00005: val_auroc improved from 0.72124 to 0.73476, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 331ms/step - loss: 0.4476 - auroc: 0.7058 - val_loss: 0.4355 - val_auroc: 0.7348\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4338 - auroc: 0.7183\n",
      "Epoch 00006: val_auroc improved from 0.73476 to 0.74060, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 331ms/step - loss: 0.4338 - auroc: 0.7183 - val_loss: 0.4209 - val_auroc: 0.7406\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4240 - auroc: 0.7261\n",
      "Epoch 00007: val_auroc improved from 0.74060 to 0.74570, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 332ms/step - loss: 0.4240 - auroc: 0.7261 - val_loss: 0.4139 - val_auroc: 0.7457\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4157 - auroc: 0.7337\n",
      "Epoch 00008: val_auroc improved from 0.74570 to 0.74840, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 331ms/step - loss: 0.4157 - auroc: 0.7337 - val_loss: 0.4162 - val_auroc: 0.7484\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4106 - auroc: 0.7382\n",
      "Epoch 00009: val_auroc improved from 0.74840 to 0.75232, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 318ms/step - loss: 0.4106 - auroc: 0.7382 - val_loss: 0.4036 - val_auroc: 0.7523\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4065 - auroc: 0.7418\n",
      "Epoch 00010: val_auroc improved from 0.75232 to 0.75296, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 315ms/step - loss: 0.4065 - auroc: 0.7418 - val_loss: 0.4045 - val_auroc: 0.7530\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4029 - auroc: 0.7456\n",
      "Epoch 00011: val_auroc improved from 0.75296 to 0.75507, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 317ms/step - loss: 0.4029 - auroc: 0.7456 - val_loss: 0.3974 - val_auroc: 0.7551\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4004 - auroc: 0.7470\n",
      "Epoch 00012: val_auroc improved from 0.75507 to 0.75773, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 318ms/step - loss: 0.4004 - auroc: 0.7470 - val_loss: 0.3951 - val_auroc: 0.7577\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.4000 - auroc: 0.7490\n",
      "Epoch 00013: val_auroc did not improve from 0.75773\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.4000 - auroc: 0.7490 - val_loss: 0.4045 - val_auroc: 0.7544\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3994 - auroc: 0.7511\n",
      "Epoch 00014: val_auroc did not improve from 0.75773\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3994 - auroc: 0.7511 - val_loss: 0.4132 - val_auroc: 0.7544\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3945 - auroc: 0.7544\n",
      "Epoch 00015: val_auroc did not improve from 0.75773\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3945 - auroc: 0.7544 - val_loss: 0.3945 - val_auroc: 0.7532\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3921 - auroc: 0.7564\n",
      "Epoch 00016: val_auroc improved from 0.75773 to 0.75789, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 316ms/step - loss: 0.3921 - auroc: 0.7564 - val_loss: 0.3901 - val_auroc: 0.7579\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3894 - auroc: 0.7588\n",
      "Epoch 00017: val_auroc improved from 0.75789 to 0.75821, saving model to model32.h5\n",
      "98/98 [==============================] - 32s 322ms/step - loss: 0.3894 - auroc: 0.7588 - val_loss: 0.3893 - val_auroc: 0.7582\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3871 - auroc: 0.7605\n",
      "Epoch 00018: val_auroc did not improve from 0.75821\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3871 - auroc: 0.7605 - val_loss: 0.3943 - val_auroc: 0.7543\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3849 - auroc: 0.7637\n",
      "Epoch 00019: val_auroc improved from 0.75821 to 0.76276, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 316ms/step - loss: 0.3849 - auroc: 0.7637 - val_loss: 0.3868 - val_auroc: 0.7628\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3839 - auroc: 0.7634\n",
      "Epoch 00020: val_auroc improved from 0.76276 to 0.76446, saving model to model32.h5\n",
      "98/98 [==============================] - 31s 316ms/step - loss: 0.3839 - auroc: 0.7634 - val_loss: 0.3871 - val_auroc: 0.7645\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3809 - auroc: 0.7683\n",
      "Epoch 00021: val_auroc did not improve from 0.76446\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3809 - auroc: 0.7683 - val_loss: 0.3858 - val_auroc: 0.7608\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3801 - auroc: 0.7693\n",
      "Epoch 00022: val_auroc did not improve from 0.76446\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3801 - auroc: 0.7693 - val_loss: 0.3909 - val_auroc: 0.7609\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3784 - auroc: 0.7717\n",
      "Epoch 00023: val_auroc did not improve from 0.76446\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3784 - auroc: 0.7717 - val_loss: 0.3860 - val_auroc: 0.7644\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3777 - auroc: 0.7720\n",
      "Epoch 00024: val_auroc did not improve from 0.76446\n",
      "98/98 [==============================] - 31s 313ms/step - loss: 0.3777 - auroc: 0.7720 - val_loss: 0.3840 - val_auroc: 0.7641\n",
      "Epoch 25/30\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.3757 - auroc: 0.7736\n",
      "Epoch 00025: val_auroc did not improve from 0.76446\n",
      "98/98 [==============================] - 31s 314ms/step - loss: 0.3757 - auroc: 0.7736 - val_loss: 0.3832 - val_auroc: 0.7608\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_3_2.compile(optimizer=Adam(learning_rate=0.0009), loss='categorical_crossentropy', metrics=[auroc])\n",
    "history_3_2 = model_3_2.fit([x_train_essay,rest_features_train], y_binary_train, batch_size=900, epochs=30, verbose=1,callbacks=callbacks_1, validation_data=([x_test_essay,rest_features_test], y_binary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 98ms/step - loss: 0.3825 - auroc: 0.7700\n",
      "Test loss: 0.38254308700561523\n",
      "Test AUC: 0.7700203061103821\n"
     ]
    }
   ],
   "source": [
    "score = model_3_2.evaluate([x_test_essay,rest_features_test], y_binary_test, batch_size=700)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test AUC:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 </br>\n",
    "- Test AUC : 0.77 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
